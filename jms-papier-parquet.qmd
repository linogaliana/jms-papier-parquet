---
title: "Le format Parquet pour la diffusion de données : un choix technique au service des utilisateurs"
authors:
  - name: Cédric Bobinec
    affiliation: Insee, Pôle ODL
    email: <cedric.bobinec@insee.fr>
  - name: Lino Galiana
    affiliation: Insee, Direction des statistiques démographiques et sociales
    email: <lino.galiana@insee.fr>
keywords: # 6 maximum
  - open data
  - open source
  - diffusion de données
domains: # utiliser la nomenclature du site (menu déroulant) 
  - Communication, open science
  - Open data, documentation, métadonnées, mise à disposition
resume: |
    La diffusion et l'exploitation de microdonnées se heurte souvent à des freins techniques liés au volume des fichiers et à la complexité de leur exploitation. Le format \texttt{Parquet}, issu à l'origine de l’écosystème big data mais s'étant diffusé bien au-delà de cette sphère, s’impose comme une alternative efficace aux formats traditionnels comme le CSV. 
 

    Le format Parquet présente, pour les statisticiens et data scientists, des propriétés idéales. Il s'agit d'un format \textit{open source}, associé à un écosystème dynamique principalement articulé autour des projets Arrow et DuckDB. Par rapport au \texttt{CSV}, le format \texttt{Parquet} permet d'obtenir des fichiers légers, rapides à lire et avec des métadonnées permettant aux producteurs de données de mieux guider les réutilisateurs de données. Ce format est bien intégré dans les principaux langages de traitement de données (\texttt{R}, \texttt{Python}, \texttt{Javascript}) ce qui facilite, dans les faits, l’exploitation de données sans nécessiter d’outils propriétaires ou de ressources machines importantes.

    Cette présentation reviendra sur les expériences de diffusion de données au format Parquet par l'Insee à partir des exemples des données anonymisées du recensement de la population ou la base des équipements publics. Ce choix technique, associé à un accompagnement des utilisateurs (guides, exemples de code), a permis d'accroître l'usage de ces sources qui étaient, auparavant, complexes à valoriser.  Cette communication évoquera également les implications pratiques de ce choix technique, les bonnes pratiques à mettre en oeuvre lors de l'adoption de ce format de données et les opportunités d'innovation permises par ce format.
abstract: |
    (Texte en anglais de 5 à 10 lignes)\ 
    `#lorem(30)`{=typst}
bibliography: references.bib
bibliographystyle: harvard-cite-them-right
format:
  jms2025-typst:
    logo:
      location: center-top # ne pas modifier
      width: 160mm # ne pas modifier
    mainfont: "tex gyre heros" # peut être modifié pour "calibri" sous windows, voir le script install-fonts.sh
    footer: 15^e^ édition des journées de méthodologie statistique de l'Insee (JMS 2025) # ne pas modifier
    section-numbering: 1.1.1.
    keep-typ: true
---


# Introduction


Parmi les nombreuses innovations techniques ayant émergé au cours des années 2010 dans l'écosystème de la donnée, le format `Parquet` fait parti de celles ayant connues une adoption large parmi les _data scientists_ et statisticiens. Initialement conçu pour répondre à aux besoins des plateformes _big data_ comme [Hadoop](https://fr.wikipedia.org/wiki/Hadoop) - réduction du volume de stockage et du temps de lecture des données - celui-ci s'est diffusé au-delà de cette sphère et a même survécu à la plupart des technologies _big data_ qui sont, depuis, tombées en désuétude [@Tigani_2023].

Plusieurs acteurs majeurs de l'écosystème de l'intelligence artificielle ont fait de `Parquet` la pierre angulaire de leur lac de données. Dans le domaine de la diffusion de données, où se rencontrent des publics aux attentes et compétences techniques diverses, l'adoption de ce format a été plus tardive. Si la plateforme communautaire `data.gouv` recense depuis plusieurs années des `Parquet` créés par des utilisateurs experts pour faciliter l'usage de certains jeux de données, la statistique publique ne s'est mise à diffuser, d'elle-même, que récemment des fichiers `Parquet`. Suite à la publication en 2023 des données détaillées du répertoire électoral unique (REU) puis du recensement de la population (RP), ce format est devenu, du fait de l'accueil positif qu'il a reçu, le moyen privilégié de diffuser des données volumineuses en _open data_ pour la statistique publique. 

Cet article vise à offrir un complément au retour d'expérience présenté lors des Journées de méthodologie statistique (JMS) 2025. Par rapport à celui-ci, cet article présentera plus extensivement les enjeux techniques qui expliquent la popularité de ce format bien qu'elles ne soient pas propres aux problématiques de diffusion de données (@sec-contexte). Cela permettra de justifier la manière dont ce choix technique rend possible de nombreuses exploitations. Nous reviendrons ensuite sur les enjeux liés à la diffusion de données publiques en `Parquet` et proposerons quelques bonnes pratiques pour leur mise à disposition et l'accompagnement des utilisateurs (@sec-retex). Enfin, nous évoquerons les perspectives qu'ouvre une adoption plus large de ce format pour la diffusion et le partage de données (@sec-perspectives).


# Retour sur les proriétés techniques du format `Parquet` {#sec-contexte}


## Les deux approches : le fichier ou la base de données

Pour bien comprendre la manière dont `Parquet`, ou des variantes de celui-ci, s'est imposé comme le format de prédilection des acteurs de l'écosystème de la donnée, il est nécessaire de revenir sur les enjeux techniques de celui-ci. Pour remettre en perspective cette solution technique, il convient de revenir sur la dualité existant dans le domaine du stockage et de la mise à disposition de donnéesentre les base de données ou les fichiers. Ces deux logiques diffèrent fortement dans la manière dont l'information est organisée, accessible et modifiable par un client qui la consomme. 

### Les bases de données relationnelles {.unnumbered}

Les bases de données relationnelles servent, au moins depuis les années 1970, dans de nombreux entrepôts de données à organiser l'information lorsqu'elle prend une forme tabulaire[^nosql]. Il s'agit d'une logique systémique. L'accès à la donnée (en lecture comme en écriture) est gérée par un système de gestion de base de données (SGBD) qui permet :

- de stocker des ensembles cohérents de données,
- d'en permettre la mise à jour (ajout, suppression, modification),
- d'en contrôler l'accès (droits utilisateurs, types de requêtes, etc.).


Dans ces systèmes, les données sont organisées en tables reliées par des relations, suivant généralement un schéma en étoile. Par exemple, une table longitudinale de données individuelles est reliée d'un côté à une table d'identifiants ménage uniques et de l'autre à une table d'identifiants entreprise unique. 

Le logiciel associé à la base de données fait ensuite le lien entre ces tables, souvent par le biais de requêtes `SQL`. L'un des logiciels les plus efficaces dans ce domaine est [`PostgreSQL`](https://www.postgresql.org/). 

[^nosql]: La diversification des données produites, notamment le fait que de nouveaux objets puissent être considérés comme de la donnée (texte, image, vidéo...), fait que d'autres approches de bases de données se sont développées pour les données non structurées où la nature coercitive du schéma en étoile n'était pas idéal. On parle pour désigner ce système _NoSQL_ (_"not only SQL"_). Comme les enjeux liés à ce type de source sont particuliers, et qu'ils concernent peu la diffusion de données de statistique publique, qui par essence est principalement tabulaire, nous n'évoquerons pas ces enjeux. 

### Les fichiers {.unnumbered}

La deuxième approche est celle du fichier qui consiste à organiser l'information dans un document stocké de manière autonome sur un système de fichiers. Contrairement à la base de données, le fichier ne repose pas sur un moteur logiciel chargé d’en gérer les accès ou la cohérence : il contient directement la donnée, dans un format plus ou moins structuré selon les besoins.

Cette approche favorise la simplicité et la portabilité. Les fichiers peuvent être partagés, copiés ou archivés sans dépendre d’une infrastructure spécifique. Ils ne nécessitent pas l'installation ou le maintien d’un logiciel de gestion spécialisé comme `PostgreSQL` : un simple _file system_, déjà présent sur tout système d’exploitation, suffit à y accéder. Leur lecture ne nécessite qu'un outil adapté au format utilisé. 

Le succès croissant des fichiers dans l’écosystème de la data science s’explique par plusieurs facteurs techniques et pratiques qui les rendent particulièrement adaptés aux usages analytiques modernes.

En premier lieu, les fichiers sont beaucoup plus légers à manipuler que les bases de données. Ils ne nécessitent pas l'installation ou le maintien d’un logiciel de gestion spécialisé : un simple _file system_, déjà présent sur tout système d’exploitation, suffit à y accéder. En ce qui concerne la lecture, il s'agit d'avoir un outil adapté au format utilisé. Ceci rend l'approche par les fichiers particulièrement souple et explique le succès de formats plats comme le `CSV` (lisible par n'importe quel éditeur de texte), `JSON` (lisible par n'importe quel navigateur web) ou, plus récemment, le format `Parquet`.


La principale raison pour laquelle les fichiers sont préférables aux SGBD pour les statisticiens et _data scientists_ réside dans la nature des opérations qu'ils effectuent. Les bases de données relationnelles prennent tout leur sens lorsqu'il s'agit de gérer des écritures fréquentes ou des mises à jour complexes sur des ensembles de données structurés[^acid], c'est-à-dire dans une logique applicative où la donnée évolue continuellement (ajout, modification, suppression). À l'inverse, dans un contexte analytique, on se contente généralement de lire et de manipuler temporairement des données sans modifier la source. L'objectif est d’interroger, d’agréger, de filtrer pour une valorisation annexe - pas de pérenniser des changements sur une donnée brute. Pour ce type d'usage, les fichiers, notamment dans des formats optimisés comme `Parquet`, sont parfaitement adaptés : ils offrent une lecture rapide, une portabilité élevée et n'imposent pas l'intermédiation d’un moteur de base de données.

[^acid]: Les bases de données ont une propriété dite ACID (Atomicité, Cohérence, Isolation et Durabilité) qui garantit que chaque transaction est exécutée entièrement ou pas du tout, qu’elle préserve l'intégrité des données, qu'elle ne perturbe pas les autres opérations simultanées et qu'elle reste enregistrée même en cas de défaillance du système.


## Les limites du CSV

Si les fichiers sont plus simples d'usage pour les besoins analytiques que les  bases de données, tous les formats ne se valent pas. Un format est une manière de contraindre l'organisation de la donnée ce qui conditionnera ultérieurement les possibilités d'exploitation, de stockage et d'interopérabilité.

L'interopérabilité, c'est-à-dire la capacité à ingérer la donnée sans imposer un logiciel, est une propriété désirable d'un fichier de donnée. A cet égard, les formats propriétaires, par exemple le `.sas7bdat` ou le `.xlsx` peuvent poser problème. Bien qu'il existe des librairies dans les logiciels statistiques _open source_ pour lire ce type de fichier, elles ne sont pas forcément optimisées et l'intégrité de la donnée n'est pas garantie même si, en général, l'importation ne pose pas de problème. 

Les formats ouverts et documentés garantissent la possibilité pour tout utilisateur, quel que soit son environnement logiciel, de lire et de traiter les données sans dépendre d'un éditeur particulier. Du fait de sa simplicité, le format `CSV` s'est imposé comme un standard de partage de données. Son principal avantage est qu'il est à la fois lisible par un humain (un simple éditeur de texte permettant de l'ouvrir) et par des programmes informatiques (du fait de sa structure tabulaire). 

Cependant, ce format, bien qu’ouvert et universellement lisible, présente de nombreuses limites techniques. En premier lieu, le `CSV` n'est pas un format compressé donc les fichiers peuvent rapidement devenir volumineux. Ce format n'embarque pas non plus de métadonnées pouvant guider la lecture du fichier, en particulier les types de variables. 

Ces limites expliquent l'émergence de formats plus modernes, capables de combiner ouverture, efficacité et richesse structurelle — au premier rang desquels le format Parquet.

## Le format `Parquet`

Sa principale caractéristique est d'être un format orienté colonne. Cela signifie que, contrairement au CSV, les données de chaque colonne sont stockées dans des blocs séparés [@fig-parquet]. Cela permet :

- de charger uniquement les colonnes utiles à une analyse ;
- de compresser plus efficacement les données ;
- d’accélérer significativement les requêtes sélectives.

L'organisation en colonnes du format Parquet permet, à partir des métadonnées décrivant la taille et la structure des blocs (ou _row groups_), de sélectionner directement les colonnes utiles à l’analyse et d’ignorer les segments non pertinents. Cette indexation interne optimise la lecture sélective et limite les accès disque.

![Organisation de la donnée dans différents formats (empruntée à @mise_en_prod)](https://ensae-reproductibilite.github.io/website/columnar-storage.png){#fig-parquet}

Une autre propriété importante du format `Parquet` est sa capacité native à produire des fichiers partitionnés, c'est à dire à distribuer un fichier de données selon une ou plusieurs clés de partitionnement. Dans la majorité des cas, les traitements statistiques ne concernent pas l'ensemble des données d'une source de données : on voudra souvent restreindre les calculs à une zone géographique, une fenêtre temporelle, etc. Si les données à traiter sont disponibles sous la forme d'un unique fichier, il faudra généralement charger l'ensemble des données en mémoire — a minima, les colonnes pertinentes — pour réaliser les traitements. A contrario, `Parquet` offre la possibilité native de partitionner un jeu de données selon une variable de filtrage fréquente (à la manière d'un index dans une base `SQL`) ce qui permet d'optimiser les traitements et d'accroître encore leur efficience (@fig-parquet-partitions). Dans cet exemple, effectuer une requête faisant intervenir seulement un ou quelques mois de données serait très efficient dans la mesure où seules les partitions pertinentes auront besoin d'être chargées en mémoire. Là encore, cette propriété rend le format `Parquet` particulièrement pertinent pour les traitements analytiques qui caractérisent les applications de *data science* [@dondon2023quels].

![Représentation sur le *filesystem* d'un fichier `Parquet` partitionné selon deux clés : l'année et le mois (empruntée à @mise_en_prod)](https://ensae-reproductibilite.github.io/website/parquet-partitions.png){#fig-parquet-partitions fig-align="center" height=300}

Ces différentes propriétés techniques (compression, rapidité en lecture, typage...) font que le format `Parquet` n’est pas réservé aux architectures _big data_ ou aux données volumineuses. Toute personne prête à utiliser un langage de programmation statistique _open source_ comme `R` ou `Python` et ayant une utilisation intensive de données pourra bénéficier de ses qualités. Pour profiter pleinement de ses propriétés, il est recommandé de privilégier certaines librairies _open source_ très bien intégrées à `R` ou `Python`.


## Les librairies pour lire efficacement des `Parquet`

Le format `Parquet` rend le stockage de données au format tabulaire beaucoup plus efficient. Mais pour pleinement bénéficier de cette structure de données, il est également nécessaire de s'intéresser à l'étape suivante : le traitement des données en mémoire.

Deux outils majeurs ont émergé à cette fin au cours des dernières années. Le premier est [`Apache Arrow`](https://arrow.apache.org/), un format tabulaire de données en mémoire interopérable entre de nombreux langages (`Python`, `R`, `Java`, etc.). Le second est [`DuckDB`](https://duckdb.org/), un système de base de données portable et interopérable permettant de requêter des sources de données de nature très variée (@fig-duckdb-multisrc). Ces deux outils, bien que techniquement très différents en termes d'implémentation, présentent des avantages et des gains de performance semblables. D'abord, ils sont tous deux orientés-colonne et travaillent ainsi en synergie avec le format `Parquet`, dans la mesure où ils font persister les bénéfices de ce format de stockage dans la mémoire (@fig-arrow-memory).

![Représentation en mémoire des données au format `Arrow` (empruntée à @mise_en_prod)](https://ensae-reproductibilite.github.io/website/arrow-memory.png){#fig-arrow-memory fig-align="center" height=400}

Ces deux librairies, comme l'illustre la @fig-arrow-memory,  font perdurer en mémoire les avantages du format `Parquet` pour le stockage. Elles permettent également d'exploiter les avancées des processeurs récents en matière de vectorisation des opérations. Dans l'exemple de la @fig-arrow-memory, la représentation en colonne des données dans la mémoire permet à la requête de filtrage sur les données de la colonne `session_id` d'être beaucoup plus efficiente que dans un format en mémoire traditionnel. Cette approche optimisée permet d'ailleurs de lire des données de grande dimension, qui peuvent excéder largement la mémoire vive de l'ordinateur ou le serveur consommant la donnée [@muehleisen_2025_lost_decade_small_data].

Par ailleurs, `Arrow` comme `DuckDB` permettent tous deux d'augmenter considérablement les performances des requêtes sur les données grâce à l'utilisation de la *lazy evaluation* ("évaluation paresseuse"). Là où les opérations sur des données sont généralement exécutées de manière linéaire par les langages de programmation - par exemple, sélectionner des colonnes et/ou filtrer des lignes, puis calculer de nouvelles colonnes, puis effectuer des agrégations, etc. - `Arrow` et `DuckDB` exécutent quant à eux ces dernières selon un plan d'exécution pré-calculé qui optimise de manière globale la chaîne de traitements. Dans ce paradigme, les calculs sont non seulement beaucoup plus performants, mais également beaucoup plus efficients dans la mesure où ils n'impliquent de récupérer que les données effectivement nécessaires pour les traitements demandés. Ces innovations permettent ainsi d'envisager des traitements basés sur des données dont le volume total dépasse la mémoire RAM effectivement disponible sur une machine.

Un avantage majeur de `DuckDB` est sa capacité à requêter de manière standardisée des sources de données très variées. `DuckDB` étant un format de base de données en mémoire, il est naturellement très adapté au requêtage de bases de données relationnelles (comme PostgreSQL ou MySQL). Mais ce *framework* peut également requêter de la même manière des fichiers de données (`CSV`, `Parquet`, etc.), qu'ils soient locaux ou stockés dans le *cloud*. Ceci peut être illustré avec quelques exemples progressifs. 

Imaginons qu'on s'intéresse au répertoire Sirene et qu'on n'ait besoin que des variables `siret`, `activitePrincipaleEtablissement` et qu'on ne souhaite conserver que les établissements actifs (`etatAdministratifEtablissement` a la modalité `A`). 

Si les données sont stockées au format CSV en local et qu'on utilise `DuckDB` via `Python`, il suffit de faire:

```{=typst} 
#titled-raw-block(filename: "Importer un CSV avec DuckDB et Python", 
  raw(lang: "Python", "import duckdb
df = duckdb.sql(\"\"\"
  FROM read_csv_auto('chemin_local/stock.csv')
  SELECT siret, activitePrincipaleEtablissement
  WHERE etatAdministratifEtablissement == 'A'
\"\"\"
).to_df()
")
)
```

Si on bascule sur un format `Parquet`, le code est presque inchangé:

```{=typst} 
#titled-raw-block(filename: "Importer un Parquet avec DuckDB et Python", 
  raw(lang: "Python", "import duckdb
df = duckdb.sql(\"\"\"
  FROM read_parquet('chemin_local/stock.parquet')
  SELECT siret, activitePrincipaleEtablissement
  WHERE etatAdministratifEtablissement == 'A'
\"\"\"
).to_df()
")
)
```

Maintenant, si le fichier se trouve en ligne, par exemple sur la plateforme _data.gouv_, le code de lecture est presque inchangé. Il est seulement nécessaire d'ajouter l'extension `httpfs` pour pouvoir lire des données non stockées en local:

```{=typst} 
#titled-raw-block(filename: "Importer un Parquet depuis internet avec DuckDB et Python", 
  raw(lang: "Python", "import duckdb

url = \"https://object.files.data.gouv.fr/data-pipeline-open/siren/stock/StockEtablissementHistorique_utf8.parquet\"
con = duckdb.connect()
con.sql(\"LOAD httpfs\")

df = con.sql(f\"\"\"
  FROM read_parquet('{url}')
  SELECT siret, activitePrincipaleEtablissement
  WHERE etatAdministratifEtablissement == 'A'
\"\"\"
).to_df()
")
)
```


Si finalement le fichier `Parquet` se trouve sur un système de stockage `S3`, par exemple sur le `SSPCloud`, le code change très peu, il suffit que le chemin de lecture commence par `s3://` après avoir créé un paramètre `s3_url_style` en amont:

```{=typst} 
#titled-raw-block(filename: "Importer un Parquet depuis S3 avec DuckDB et Python", 
  raw(lang: "Python", "import duckdb

path = \"s3://bucket/sirene.parquet\" # Chemin commence par s3://
con = duckdb.connect()
con.sql(\"LOAD httpfs; SET s3_url_style = 'path';\")

df = con.sql(f\"\"\"
  FROM read_parquet('{path}')
  SELECT siret, activitePrincipaleEtablissement
  WHERE etatAdministratifEtablissement == 'A'
\"\"\"
).to_df()
")
)
```

Puisque la lecture et le traitement se font par le biais d'une requête SQL, il est tout à fait possible de directement récupérer un agrégat. Par exemple pour dénombrer les entreprises actives pour chaque APET:

```{=typst} 
#titled-raw-block(filename: "Traiter directement un Parquet depuis S3 avec DuckDB et Python", 
  raw(lang: "Python", "import duckdb

path = \"s3://bucket/sirene.parquet\" # Chemin commence par s3://
con = duckdb.connect()
con.sql(\"LOAD httpfs; SET s3_url_style = 'path';\")

df = con.sql(f\"\"\"
  FROM read_parquet('{path}')
  SELECT count(siret) 
  WHERE etatAdministratifEtablissement == 'A'
  GROUP BY activitePrincipaleEtablissement
\"\"\"
).to_df()
")
)
```

![_DuckDB_, un outil universel pour lire des données (empruntée à @mise_en_prod)](https://ensae-reproductibilite.github.io/website/duckdb-multisrc.png){#fig-duckdb-multisrc fig-align="center" height=400}


# La diffusion de données au format Parquet {#sec-retex}

Les premières expérimentations de diffusion de données au format `Parquet` représentaient des cas d'usage idéaux pour tester la réception de ce format par les utilisateurs. Le fichier de diffusion des données détaillées du recensement de la population représente, par exemple, environ 20 millions d'observations pour 80 variables. Ce fichier est à destination d'utilisateurs experts qui désirent construire des statistiques à façon à partir de croisements qui n'existent pas dans les données agrégées. 

Ce fichier était historiquement diffusé aux formats `dBase` (un SGBD dont les origines remontent aux années 1980) et `CSV`. Un millésime approchait, au format CSV, les 5Go sur disque. Il nécessitait donc des ressources mémoire conséquentes pour être lu dans un logiciel statistique comme `R` et `Python` puisque ce format ne permet pas une lecture parcimonieuse du fichier. 

La comparaison des performances en lecture de ces données au format CSV et Parquet est sans appel[^reproductibilite]. La mise à disposition d'un fichier n'oblige plus l'utilisateur à faire preuve d'ingéniosité pour découper son fichier de sorte à pouvoir le lire par bloc: cette propriété native du fichier `Parquet` permet aux utilisateurs de se concentrer sur l'exploitation des données. Les utilisateurs du fichier ne sont généralement pas intéressés par l'intégralité des colonnes: l'orientation colonne de `Parquet` permettant de n'importer que les variables utiles à l'analyse représente un gain important de confort pour ces utilisateurs. Pour ceux ne s'intéressant qu'à des sous-ensemble de données, par exemple pour des études locales, les retours d'expérience, notamment ceux de @mauviere_2024_preparer_parquet, ont permis de prendre conscience du rôle important du tri des variables pour l'optimisation des filtres.


Pour aider les utilisateurs à s'approprier ce format, un guide d'utilisation des données a été publié sur le blog du réseau des _data scientists_ de la statistique publique, le [SSPHub (ssphub.netlify.app/)](https://ssphub.netlify.app/), pour accompagner la diffusion. 

![Exemple de comparaison des performances du format `Parquet` dans plusieurs cas d'usage sur les données détaillées du recensement de la population diffusées par l'Insee en 2023](https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/slides/img/tableau-perf-parquet.png){#fig-tableau-parquet width="80%"}

[^reproductibilite]: Ce tableau est produit dans le cadre de la formation aux bonnes pratiques `R` et `Git` développée par l'Insee et faisant la promotion du format `Parquet` pour le stockage de données. Pour reproduire celui-ci, se rendre sur [inseefrlab.github.io/formation-bonnes-pratiques-git-R/slides/complete.html](https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/). 


Outre une utilisation efficace des ressources, la diffusion dans ce format appporte aussi du confort aux utilisateurs, notamment en montrant comment lire ces données avec `DuckDB`. Comme cette librairie est simple d'usage, le public cible de la diffusion de ces données, majoritairement utilisateur de `R` ou `Python`, pouvait ainsi s'approprier rapidement ce nouveau format. Pour illustrer cette simplicité d'usage, prenons pour exemple un besoin précis: dénombrer les logements niçois dont les occupants ont emmenagé depuis 2020. En `Python`, cela se traduit par la requête SQL:

```{=typst} 
#titled-raw-block(filename: "Un exemple d'utilisation des données diffusées par l'Insee avec DuckDB et Python", 
  raw(lang: "Python", "import duckdb

url = \"https://www.data.gouv.fr/api/1/datasets/r/fdf5b008-054b-4065-a819-3eb3313f8be7\"
duckdb.sql(
  f\"FROM read_parquet(\\\"{url}\\\")\"
  \"SELECT COUNT(*) \"
  \"WHERE COMMUNE = '06088' and CAST(AEMM AS int) > 2020\"
)

).to_df()
")
)
```


En `R`, il est possible d'écrire un code très proche quoiqu'un peu plus verbeux. 

```{=typst} 
#titled-raw-block(filename: "Un exemple d'utilisation des données diffusées par l'Insee avec DuckDB et R", 
  raw(lang: "Python", "url <- \"https://www.data.gouv.fr/api/1/datasets/r/fdf5b008-054b-4065-a819-3eb3313f8be7\"
con <- DBI::dbConnect(duckdb::duckdb(), \":memory:\")
query <- glue::glue(
  \"FROM read_parquet(\\\"{url}\\\") \",
  \"SELECT COUNT(*) \",
  \"WHERE COMMUNE = '06088' and CAST(AEMM AS int) > 2020\"
)
dbGetQuery(con, query)
")
)
```

Il est néanmoins possible de directement utiliser la syntaxe très confortable de `dplyr` qui permet, par le biais du _package_ `dbplyr`, de déléguer les calculs au moteur d'exécution de `duckdb`

```{=typst} 
#titled-raw-block(filename: "Un exemple d'utilisation des données diffusées par l'Insee avec DuckDB et R via dplyr", 
  raw(lang: "Python", "library(dplyr)
  
url <- \"https://www.data.gouv.fr/api/1/datasets/r/fdf5b008-054b-4065-a819-3eb3313f8be7\"
con <- DBI::dbConnect(duckdb::duckdb(), \":memory:\")
df <- tbl(con, glue::glue(\"read_parquet(\\\"{url}\\\")\"))
df |>
  filter(COMMUNE = '06088', AEMM > 2020) |>
  tally() |>
  collect()
")
)
```


Puisque les premières diffusions de `Parquet` en 2023 ont été appréciées par les utilisateurs des fichiers détails, ce format a été proposé dans la diffusion d'autres sources canonique de l'Insee. La @tbl-download illustre les principales sources diffusées par l'Insee sur `data.gouv` en `Parquet` et le nombre de téléchargements de celles-ci. L'Insee n'est d'ailleurs pas la seule institution de la statistique publique à avoir adopté ce format de diffusion pour certaines sources volumineuses. Le service statistique ministériel du ministère de l'intérieur (le SSMSI) diffuse, depuis 2024, par exemple, les [bases statistiques de la délinquance](https://www.data.gouv.fr/datasets/bases-statistiques-communale-departementale-et-regionale-de-la-delinquance-enregistree-par-la-police-et-la-gendarmerie-nationales/) dans ce format de données. 


```{python}
#| label: tbl-download
#| tbl-cap: "Principales sources de données diffusées par l'Insee sur data.gouv au format Parquet"
#| echo: false
import duckdb
from great_tables import *

tab = duckdb.sql("""
FROM read_csv_auto('/home/onyxia/work/jms-papier-parquet/consult_data_gouv_parquet.csv')
SELECT
  title,
  '[' || page || '](' || page || ')' AS page,
  metrics_resources_downloads
WHERE organization_name='Institut national de la statistique et des études économiques (Insee)'
ORDER BY metrics_resources_downloads DESC
""").to_df()

tab["page"] = tab["page"].str.replace("https://www.", "")

tab = (
    GT(tab)
    .fmt_number("metrics_resources_downloads", decimals = 0, sep_mark = " ")
    .fmt_markdown("page")
    .cols_label(
        {
            "title": "Base",
            "page": md("Page sur `data.gouv`"),
            "metrics_resources_downloads": "Téléchargements"
        }
    )
    .tab_source_note(source_note=md("*Note*: Les statistiques sur le nombre de téléchargement représentent ici tous les produits publiés sur la page en question, pas exclusivement les fichiers `Parquet`"))
)

tab
```

L'acculturation progressive des utilisateurs comme des producteurs d'_open data_ permet de prendre conscience de certaines bonnes pratiques à avoir dans la préparation du `Parquet` permettant de bénéficier des optimisations de ce fichier. Les principaux enjeux à avoir à l'esprit sont résumés par @mauviere_2024_preparer_parquet: définir des types de colonnes adéquats aux données représentées, trier le `Parquet` en fonction de variables qui servent fréquemment de filres, privilégier une compression `zstd` à `snappy` (défaut), mettre à disposition ce parquet sur des serveurs autorisant les [_range request_](https://developer.mozilla.org/fr/docs/Web/HTTP/Guides/Range_requests)... 


# Perspectives {#sec-perspectives}

Si ces premières expériences de diffusion de données au format `Parquet` ont permis d'illustrer l'apport de ce choix pour les _data scientists_, statisticiens et géomaticiens utilisant des données détaillées, ce n'est pas la seule communauté pouvant bénéficier de ce format. Les spécialistes de _data visualisation_, une communauté plus proche du développement web, utilisant plutôt le langage `Javascript` peuvent également bénéficier de ce format. Grâce au portage de `DuckDB` en _Web Assembly_[^wasm], il est possible de construire des _data visualisations_ interactives capables de lire directement des blocs de données issus d’un fichier `Parquet` volumineux. Ces blocs peuvent être sélectionnés dynamiquement via une interface utilisateur, sans nécessiter de serveur intermédiaire comme c'est le cas pour les solutions type `Shiny` ou `Steamlit`. Lorsque le fichier `Parquet` est structuré de manière optimale, la lecture partielle devient extrêmement efficace, ce qui permet d'avoir une _data visualisation_ très réactive même avoir des données de grande dimension[^atelier].  

[^wasm]: _WebAssembly_ est une technologie complémentaire à `JavaScript` : elle permet d’exécuter dans le navigateur du code compilé à partir d’autres langages (comme `C++`, `Rust` ou `Go`). `JavaScript` reste le langage d’orchestration ; il charge, initialise et communique avec les modules _WebAssembly_, tandis que ces derniers exécutent les calculs lourds de manière plus efficace. Dans le contexte de la _data visualisation_, cela signifie qu’un moteur analytique comme `DuckDB` peut tourner directement dans le navigateur, sans serveur, tout en interagissant avec les bibliothèques `JavaScript` de visualisation (par exemple `D3.js` ou `Vega-Lite`).

[^atelier]: Pour en apprendre plus sur ce sujet, se reporter vers un atelier sur le sujet animé dans le cadre du réseau des data scientists de la statistique publique: [https://ssphub.github.io/ssphub-ateliers-parquet/](https://ssphub.github.io/ssphub-ateliers-parquet/).

Le format `Parquet` n'a pas vocation à devenir le principal mode de diffusion des données en raison du profil des utilisateurs du site _insee.fr_ dont les besoins portent majoritairement sur des données déjà agrégées et directement exploitables sans outil statistique. Il peut néanmoins constituer, dans de nombreux cas d’usage analytique, une alternative pertinente aux API pour l’accès aux données. Poussée à son extrême, la logique de consommation frugale d'un gros fichier permet d'envisager un modèle de diffusion de données faisant de fichiers `Parquet` de véritables API de données où, au lieu de définir des requêtes d'accès dans un `JSON`, celles-ci seraient définies à partir de requêtes SQL. C'est cette logique qu'adoptent certains acteurs centraux de l'écosystème `Parquet` ayant des besoins analytiques importants en proposant des lacs de données s'appuyant sur `Parquet`, en remplacement d'entrepôts de données reposant sur la logique des bases de données.  










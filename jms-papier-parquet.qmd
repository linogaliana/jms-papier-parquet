---
title: "Le format Parquet pour la diffusion de données : un choix technique au service des utilisateurs"
authors:
  - name: Cédric Bobinec
    affiliation: Insee, Pôle ODL
    email: <cedric.bobinec@insee.fr>
  - name: Lino Galiana
    affiliation: Insee, Direction des statistiques démographiques et sociales
    email: <lino.galiana@insee.fr>
keywords: # 6 maximum
  - open data
  - open source
  - diffusion de données
domains: # utiliser la nomenclature du site (menu déroulant) 
  - Communication, open science
  - Open data, documentation, métadonnées, mise à disposition
resume: |
    La diffusion et l'exploitation de microdonnées se heurte souvent à des freins techniques liés au volume des fichiers et à la complexité de leur exploitation. Le format \texttt{Parquet}, issu à l'origine de l’écosystème big data mais s'étant diffusé bien au-delà de cette sphère, s’impose comme une alternative efficace aux formats traditionnels comme le CSV. 
 

    Le format Parquet présente, pour les statisticiens et data scientists, des propriétés idéales. Il s'agit d'un format \textit{open source}, associé à un écosystème dynamique principalement articulé autour des projets Arrow et DuckDB. Par rapport au \texttt{CSV}, le format \texttt{Parquet} permet d'obtenir des fichiers légers, rapides à lire et avec des métadonnées permettant aux producteurs de données de mieux guider les réutilisateurs de données. Ce format est bien intégré dans les principaux langages de traitement de données (\texttt{R}, \texttt{Python}, \texttt{Javascript}) ce qui facilite, dans les faits, l’exploitation de données sans nécessiter d’outils propriétaires ou de ressources machines importantes.

    Cette présentation reviendra sur les expériences de diffusion de données au format Parquet par l'Insee à partir des exemples des données anonymisées du recensement de la population ou la base des équipements publics. Ce choix technique, associé à un accompagnement des utilisateurs (guides, exemples de code), a permis d'accroître l'usage de ces sources qui étaient, auparavant, complexes à valoriser.  Cette communication évoquera également les implications pratiques de ce choix technique, les bonnes pratiques à mettre en oeuvre lors de l'adoption de ce format de données et les opportunités d'innovation permises par ce format.
abstract: |
    (Texte en anglais de 5 à 10 lignes)\ 
    `#lorem(30)`{=typst}
bibliography: references.bib
format:
  jms2025-typst:
    logo:
      location: center-top # ne pas modifier
      width: 160mm # ne pas modifier
    mainfont: "tex gyre heros" # peut être modifié pour "calibri" sous windows, voir le script install-fonts.sh
    footer: 15^e^ édition des journées de méthodologie statistique de l'Insee (JMS 2025) # ne pas modifier
    section-numbering: 1.1.1.
    keep-typ: true
---

# Introduction

Parmi les nombreuses innovations techniques ayant émergé au cours des années 2010 dans l'écosystème de la donnée, le format `Parquet` fait parti de celles ayant connues une adoption large parmi les _data scientists_ et statisticiens. Initialement conçu pour répondre à aux besoins des plateformes _big data_ comme [Hadoop](https://fr.wikipedia.org/wiki/Hadoop) - réduction du volume de stockage et du temps de lecture des données - celui-ci s'est diffusé au-delà de cette sphère et a même survécu à la plupart des technologies _big data_ qui sont, depuis, tombées en désuétude [@Tigani_2023].

Plusieurs acteurs majeurs de l'écosystème de l'intelligence artificielle ont fait de `Parquet` la pierre angulaire de leur lac de données. Dans le domaine de la diffusion de données, où se rencontrent des publics aux attentes et compétences techniques diverses, l'adoption de ce format a été plus tardive. Si la plateforme communautaire `data.gouv` recense depuis plusieurs années des `Parquet` créés par des utilisateurs experts pour faciliter l'usage de certains jeux de données, la statistique publique ne s'est mise à diffuser, d'elle-même, que récemment des fichiers `Parquet`. Suite à la publication en 2023 des données détaillées du répertoire électoral unique (REU) puis du recensement de la population (RP), ce format est devenu, du fait de l'accueil positif qu'il a reçu, le moyen privilégié de diffuser des données volumineuses en _open data_ pour la statistique publique. 

Cet article vise à offrir un complément au retour d'expérience présenté lors des Journées de méthodologie statistique (JMS) 2025. Par rapport à celui-ci, cet article présentera plus extensivement les enjeux techniques qui expliquent la popularité de ce format bien qu'elles ne soient pas propres aux problématiques de diffusion de données (@sec-contexte). Cela permettra de justifier la manière dont ce choix technique rend possible de nombreuses exploitations. Nous reviendrons ensuite sur les enjeux liés à la diffusion de données publiques en `Parquet` et proposerons quelques bonnes pratiques pour leur mise à disposition et l'accompagnement des utilisateurs (@sec-retex). Enfin, nous évoquerons les perspectives qu'ouvre une adoption plus large de ce format pour la diffusion et le partage de données (@sec-perspectives).


# Retour sur les proriétés techniques du format `Parquet` {#sec-contexte}


## Les deux approches : le fichier ou la base de données

Pour bien comprendre la manière dont `Parquet`, ou des variantes de celui-ci, s'est imposé comme le format de prédilection des acteurs de l'écosystème de la donnée, il est nécessaire de revenir sur les enjeux techniques de celui-ci. Pour remettre en perspective cette solution technique, il convient de revenir sur la dualité existant dans le domaine du stockage et de la mise à disposition de donnéesentre les base de données ou les fichiers. Ces deux logiques diffèrent fortement dans la manière dont l'information est organisée, accessible et modifiable par un client qui la consomme. 

### Les bases de données relationnelles {.unnumbered}

Les bases de données relationnelles servent, au moins depuis les années 1970, dans de nombreux entrepôts de données à organiser l'information lorsqu'elle prend une forme tabulaire[^nosql]. Il s'agit d'une logique systémique. L'accès à la donnée (en lecture comme en écriture) est gérée par un système de gestion de base de données (SGBD) qui permet :

- de stocker des ensembles cohérents de données,
- d'en permettre la mise à jour (ajout, suppression, modification),
- d'en contrôler l'accès (droits utilisateurs, types de requêtes, etc.).


Dans ces systèmes, les données sont organisées en tables reliées par des relations, suivant généralement un schéma en étoile. Par exemple, une table longitudinale de données individuelles est reliée d'un côté à une table d'identifiants ménage uniques et de l'autre à une table d'identifiants entreprise unique. 

Le logiciel associé à la base de données fait ensuite le lien entre ces tables, souvent par le biais de requêtes `SQL`. L'un des logiciels les plus efficaces dans ce domaine est [`PostgreSQL`](https://www.postgresql.org/). 

[^nosql]: La diversification des données produites, notamment le fait que de nouveaux objets puissent être considérés comme de la donnée (texte, image, vidéo...), fait que d'autres approches de bases de données se sont développées pour les données non structurées où la nature coercitive du schéma en étoile n'était pas idéal. On parle pour désigner ce système _NoSQL_ (_"not only SQL"_). Comme les enjeux liés à ce type de source sont particuliers, et qu'ils concernent peu la diffusion de données de statistique publique, qui par essence est principalement tabulaire, nous n'évoquerons pas ces enjeux. 

### Les fichiers {.unnumbered}

La deuxième approche est celle du fichier qui consiste à organiser l'information dans un document stocké de manière autonome sur un système de fichiers. Contrairement à la base de données, le fichier ne repose pas sur un moteur logiciel chargé d’en gérer les accès ou la cohérence : il contient directement la donnée, dans un format plus ou moins structuré selon les besoins.

Cette approche favorise la simplicité et la portabilité. Les fichiers peuvent être partagés, copiés ou archivés sans dépendre d’une infrastructure spécifique. Ils ne nécessitent pas l'installation ou le maintien d’un logiciel de gestion spécialisé comme `PostgreSQL` : un simple _file system_, déjà présent sur tout système d’exploitation, suffit à y accéder. Leur lecture ne nécessite qu'un outil adapté au format utilisé. 

Le succès croissant des fichiers dans l’écosystème de la data science s’explique par plusieurs facteurs techniques et pratiques qui les rendent particulièrement adaptés aux usages analytiques modernes.

En premier lieu, les fichiers sont beaucoup plus légers à manipuler que les bases de données. Ils ne nécessitent pas l'installation ou le maintien d’un logiciel de gestion spécialisé : un simple _file system_, déjà présent sur tout système d’exploitation, suffit à y accéder. En ce qui concerne la lecture, il s'agit d'avoir un outil adapté au format utilisé. Ceci rend l'approche par les fichiers particulièrement souple et explique le succès de formats plats comme le `CSV` (lisible par n'importe quel éditeur de texte), `JSON` (lisible par n'importe quel navigateur web) ou, plus récemment, le format `Parquet`.


La principale raison pour laquelle les fichiers sont préférables aux SGBD pour les statisticiens et _data scientists_ réside dans la nature des opérations qu'ils effectuent. Les bases de données relationnelles prennent tout leur sens lorsqu'il s'agit de gérer des écritures fréquentes ou des mises à jour complexes sur des ensembles de données structurés[^acid], c'est-à-dire dans une logique applicative où la donnée évolue continuellement (ajout, modification, suppression). À l'inverse, dans un contexte analytique, on se contente généralement de lire et de manipuler temporairement des données sans modifier la source. L'objectif est d’interroger, d’agréger, de filtrer pour une valorisation annexe - pas de pérenniser des changements sur une donnée brute. Pour ce type d'usage, les fichiers, notamment dans des formats optimisés comme `Parquet`, sont parfaitement adaptés : ils offrent une lecture rapide, une portabilité élevée et n'imposent pas l'intermédiation d’un moteur de base de données.

[^acid]: Les bases de données ont une propriété dite ACID (Atomicité, Cohérence, Isolation et Durabilité) qui garantit que chaque transaction est exécutée entièrement ou pas du tout, qu’elle préserve l'intégrité des données, qu'elle ne perturbe pas les autres opérations simultanées et qu'elle reste enregistrée même en cas de défaillance du système.


## Les limites du CSV

Si les fichiers sont plus simples d'usage pour les besoins analytiques que les  bases de données, tous les formats ne se valent pas. Un format est une manière de contraindre l'organisation de la donnée ce qui conditionnera ultérieurement les possibilités d'exploitation, de stockage et d'interopérabilité.

L'interopérabilité, c'est-à-dire la capacité à ingérer la donnée sans imposer un logiciel, est une propriété désirable d'un fichier de donnée. A cet égard, les formats propriétaires, par exemple le `.sas7bdat` ou le `.xlsx` peuvent poser problème. Bien qu'il existe des librairies dans les logiciels statistiques _open source_ pour lire ce type de fichier, elles ne sont pas forcément optimisées et l'intégrité de la donnée n'est pas garantie même si, en général, l'importation ne pose pas de problème. 

Les formats ouverts et documentés garantissent la possibilité pour tout utilisateur, quel que soit son environnement logiciel, de lire et de traiter les données sans dépendre d'un éditeur particulier. Du fait de sa simplicité, le format `CSV` s'est imposé comme un standard de partage de données. Son principal avantage est qu'il est à la fois lisible par un humain (un simple éditeur de texte permettant de l'ouvrir) et par des programmes informatiques (du fait de sa structure tabulaire). 

Cependant, ce format, bien qu’ouvert et universellement lisible, présente de nombreuses limites techniques. En premier lieu, le `CSV` n'est pas un format compressé donc les fichiers peuvent rapidement devenir volumineux. Ce format n'embarque pas non plus de métadonnées pouvant guider la lecture du fichier, en particulier les types de variables. 

Ces limites expliquent l'émergence de formats plus modernes, capables de combiner ouverture, efficacité et richesse structurelle — au premier rang desquels le format Parquet.

## Le format `Parquet`

Sa principale caractéristique est d'être un format orienté colonne. Cela signifie que, contrairement au CSV, les données de chaque colonne sont stockées dans des blocs séparés [@fig-parquet]. Cela permet :

- de charger uniquement les colonnes utiles à une analyse ;
- de compresser plus efficacement les données ;
- d’accélérer significativement les requêtes sélectives.

L'organisation en colonnes du format Parquet permet, à partir des métadonnées décrivant la taille et la structure des blocs (ou _row groups_), de sélectionner directement les colonnes utiles à l’analyse et d’ignorer les segments non pertinents. Cette indexation interne optimise la lecture sélective et limite les accès disque.

![Organisation de la donnée dans différents formats (empruntée à @mise_en_prod)](https://ensae-reproductibilite.github.io/website/columnar-storage.png){#fig-parquet}

Une autre propriété importante du format `Parquet` est sa capacité native à produire des fichiers partitionnés, c'est à dire à distribuer un fichier de données selon une ou plusieurs clés de partitionnement. Dans la majorité des cas, les traitements statistiques ne concernent pas l'ensemble des données d'une source de données : on voudra souvent restreindre les calculs à une zone géographique, une fenêtre temporelle, etc. Si les données à traiter sont disponibles sous la forme d'un unique fichier, il faudra généralement charger l'ensemble des données en mémoire — a minima, les colonnes pertinentes — pour réaliser les traitements. A contrario, `Parquet` offre la possibilité native de partitionner un jeu de données selon une variable de filtrage fréquente (à la manière d'un index dans une base `SQL`) ce qui permet d'optimiser les traitements et d'accroître encore leur efficience (@fig-parquet-partitions). Dans cet exemple, effectuer une requête faisant intervenir seulement un ou quelques mois de données serait très efficient dans la mesure où seules les partitions pertinentes auront besoin d'être chargées en mémoire. Là encore, cette propriété rend le format `Parquet` particulièrement pertinent pour les traitements analytiques qui caractérisent les applications de *data science* [@dondon2023quels].

![Représentation sur le *filesystem* d'un fichier `Parquet` partitionné selon deux clés : l'année et le mois (empruntée à @mise_en_prod)](https://ensae-reproductibilite.github.io/website/parquet-partitions.png){#fig-parquet-partitions fig-align="center" height=300}

Ces différentes propriétés techniques (compression, rapidité en lecture, typage...) font que le format `Parquet` n’est pas réservé aux architectures _big data_. Toute personne prête à utiliser un langage de programmation statistique utilisant des jeux de données poura bénéficier de ses qualités. Dans le domaine de la diffusion, cela signifie 


## Les librairies pour lire efficacement des `Parquet`

Le format `Parquet` rend le stockage de données au format tabulaire beaucoup plus efficient. Mais pour pleinement bénéficier de cette structure de données, il est également nécessaire de s'intéresser à l'étape suivante : le traitement des données en mémoire.

Deux outils majeurs ont émergé à cette fin au cours des dernières années. Le premier est [`Apache Arrow`](https://arrow.apache.org/), un format tabulaire de données en mémoire interopérable entre de nombreux langages (`Python`, `R`, `Java`, etc.). Le second est [`DuckDB`](https://duckdb.org/), un système de base de données portable et interopérable permettant de requêter des sources de données de nature très variée (@fig-duckdb-multisrc). Ces deux outils, bien que techniquement très différents en termes d'implémentation, présentent des avantages et des gains de performance semblables. D'abord, ils sont tous deux orientés-colonne et travaillent ainsi en synergie avec le format `Parquet`, dans la mesure où ils font persister les bénéfices de ce format de stockage dans la mémoire (@fig-arrow-memory).

![Représentation en mémoire des données au format `Arrow` (empruntée à @mise_en_prod)](https://ensae-reproductibilite.github.io/website/arrow-memory.png){#fig-arrow-memory fig-align="center" height=400}

Ces deux librairies, comme l'illustre la @fig-arrow-memory,  font perdurer en mémoire les avantages du format `Parquet` pour le stockage, et permettent d'exploiter les avancées des processeurs récents en matière de vectorisation des opérations. Dans cet exemple, la représentation en colonne des données dans la mémoire permet à la requête de filtrage sur les données de la colonne `session_id` d'être beaucoup plus efficiente que dans un format en mémoire traditionnel. 

Par ailleurs, `Arrow` comme `DuckDB` permettent tous deux d'augmenter considérablement les performances des requêtes sur les données grâce à l'utilisation de la *lazy evaluation* ("évaluation paresseuse"). Là où les opérations sur des données sont généralement exécutées de manière linéaire par les langages de programmation — par exemple, sélectionner des colonnes et/ou filtrer des lignes, puis calculer de nouvelles colonnes, puis effectuer des agrégations, etc. — `Arrow` et `DuckDB` exécutent quant à eux ces dernières selon un plan d'exécution pré-calculé qui optimise de manière globale la chaîne de traitements. Dans ce paradigme, les calculs sont non seulement beaucoup plus performants, mais également beaucoup plus efficients dans la mesure où ils n'impliquent de récupérer que les données effectivement nécessaires pour les traitements demandés. Ces innovations permettent ainsi d'envisager des traitements basés sur des données dont le volume total dépasse la mémoire RAM effectivement disponible sur une machine.

Un avantage majeur de `DuckDB` est sa capacité à requêter de manière standardisée des sources de données très variées. `DuckDB` étant un format de base de données en mémoire, il est naturellement très adapté au requêtage de bases de données relationnelles (comme PostgreSQL ou MySQL). Mais ce *framework* peut également requêter de la même manière des fichiers de données (`CSV`, `Parquet`, etc.), qu'ils soient locaux ou stockés dans le *cloud*. Par exemple, imaginons qu'on s'intéresse au répertoire Sirene et qu'on n'ait besoin que des variables `siret`, `activitePrincipaleEtablissement` pour les établissements actifs (`etatAdministratifEtablissement`). Si les données sont en CSV en local et qu'on utilise `Python`, il suffit de faire

```python
import duckdb
df = duckdb.sql("""
  FROM read_csv_auto('chemin_local/stock.csv')
  SELECT siret, activitePrincipaleEtablissement
  WHERE etatAdministratifEtablissement == 'A'
"""
).to_df()
```

Si on bascule sur un format `Parquet`, le code est presque inchangé:

```python
import duckdb
df = duckdb.sql("""
  FROM read_parquet('chemin_local/stock.parquet')
  SELECT siret, activitePrincipaleEtablissement
  WHERE etatAdministratifEtablissement == 'A'
"""
).to_df()
```

Maintenant, si le fichier se trouve en ligne, par exemple sur la plateforme _data.gouv_, le code de lecture est inchangé, il est seulement nécessaire d'ajouter l'extension `httpfs` pour pouvoir lire des données non stockées en local

```python
import duckdb

con = duckdb.connect()

con.sql("LOAD httpfs")

df = con.sql("""
  FROM read_parquet('https://object.files.data.gouv.fr/data-pipeline-open/siren/stock/StockEtablissementHistorique_utf8.parquet')
  SELECT siret, activitePrincipaleEtablissement
  WHERE etatAdministratifEtablissement == 'A'
"""
).to_df()
```

Si finalement le fichier `Parquet` se trouve sur un système de stockage `S3`, par exemple sur le `SSPCloud`, le code change très peu, il suffit que le chemin de lecture commence par `s3://` après avoir créé un paramètre `s3_url_style` en amont:

```python
import duckdb

con = duckdb.connect()

con.sql("LOAD httpfs; SET s3_url_style = 'path';")

df = con.sql("""
  FROM read_parquet('s3://bucket/sirene.parquet')
  SELECT siret, activitePrincipaleEtablissement
  WHERE etatAdministratifEtablissement == 'A'
"""
).to_df()
```

Puisque la lecture et le traitement se font par le biais d'une requête SQL, il est tout à fait possible de directement récupérer un agrégat. Par exemple pour dénombrer les entreprises actives pour chaque APET:

```python
import duckdb

con = duckdb.connect()

con.sql("LOAD httpfs; SET s3_url_style = 'path';")

df = con.sql("""
  FROM read_parquet('s3://bucket/sirene.parquet')
  SELECT count(siret) 
  WHERE etatAdministratifEtablissement == 'A'
  GROUP BY activitePrincipaleEtablissement
"""
).to_df()
```


![_DuckDB_, un outil universel pour lire des données (empruntée à @mise_en_prod)](https://ensae-reproductibilite.github.io/website/duckdb-multisrc.png){#fig-duckdb-multisrc fig-align="center" height=400}


# La diffusion de données au format Parquet

Les premières expérimentations de diffusion de données au format `Parquet` représentaient des cas d'usage idéaux pour tester la réception de ce format par les utilisateurs. Le fichier de diffusion des données détaillées du recensement de la population représente, par exemple, environ 20 millions d'observations pour 80 variables. Ce fichier est à destination d'utilisateurs experts qui désirent construire des statistiques à façon à partir de croisements qui n'existent pas dans les données agrégées. 

Ce fichier était historiquement diffusé aux formats `dBase` (un SGBD dont les origines remontent aux années 1980) et ``CSV`. Un millésime représentait, au format CSV, approche des 5Go sur disque. Il nécessitait donc des ressources mémoire conséquentes pour être lu dans un logiciel statistique comme `R` et `Python` puisque ce format ne permet pas une lecture parcimonieuse du fichier. 

La comparaison des performances en lecture de ces données au format CSV et Parquet est sans appel[^reproductibilite]. La mise à disposition d'un fichier n'oblige plus l'utilisateur à faire preuve d'ingéniosité pour découper son fichier de sorte à pouvoir le lire par bloc: cette propriété native du fichier `Parquet` permet aux utilisateurs de se concentrer sur l'exploitation des données. Pour aider les utilisateurs à s'approprier ce format, un guide, publié sur le blog du [SSPHub (ssphub.netlify.app/)](https://ssphub.netlify.app/), a accompagné la diffusion. 

![Exemple de comparaison des performances du format `Parquet` dans plusieurs cas d'usage sur les données détaillées du recensement de la population diffusées par l'Insee en 2023](https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/slides/img/tableau-perf-parquet.png){#fig-tableau-parquet width="80%"}

[^reproductibilite]: Ce tableau est produit dans le cadre de la formation aux bonnes pratiques `R` et `Git` développée par l'Insee et faisant la promotion du format `Parquet` pour le stockage de données. Pour reproduire celui-ci, se rendre sur [inseefrlab.github.io/formation-bonnes-pratiques-git-R/slides/complete.html](https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/slides/complete.html). 


Outre une utilisation efficace des ressources, la diffusion dans ce format appporte aussi du confort aux utilisateurs, notamment grâce à l'usage de `DuckDB`. Cette librairie, simple d'usage depuis `R` ou `Python` permet de lire ce fichier simplement. Prenons pour exemple un besoin précis: dénombrer les logements niçois dont les occupants ont emmenagé depuis 2020. En `Python`, cela se traduit par la requête SQL:

```python
import duckdb

url = "https://www.data.gouv.fr/api/1/datasets/r/fdf5b008-054b-4065-a819-3eb3313f8be7"
duckdb.sql(
  f"FROM read_parquet(\"{url}\") "
  "SELECT COUNT(*) "
  "WHERE COMMUNE = '06088' and CAST(AEMM AS int) > 2020 "
)
```

En `R`, il est possible d'écrire un code très proche quoiqu'un peu plus verbeux. 

```r
url <- "https://www.data.gouv.fr/api/1/datasets/r/fdf5b008-054b-4065-a819-3eb3313f8be7"

con <- DBI::dbConnect(duckdb::duckdb(), ":memory:")
query <- glue::glue(
  "FROM read_parquet(\"{url}\") ",
  "SELECT COUNT(*) ",
  "WHERE COMMUNE = '06088' and CAST(AEMM AS int) > 2020 "
)
dbGetQuery(con, query)
```

Il est néanmoins possible de directement utiliser la syntaxe très confortable de `dplyr` qui permet, par le biais du _package_ `dbplyr`, de déléguer les calculs au moteur d'exécution de `duckdb`

```r
library(dplyr)

url <- "https://www.data.gouv.fr/api/1/datasets/r/fdf5b008-054b-4065-a819-3eb3313f8be7"
con <- DBI::dbConnect(duckdb::duckdb(), ":memory:")
df <- tbl(con, glue::glue("read_parquet(\"{url}\")"))
df |>
  filter(COMMUNE = '06088', AEMM > 2020) |>
  tally() |>
  collect()
```

Puisque les premières diffusions de `Parquet` en 2021 ont été appréciées par les utilisateurs des fichiers détails, ce format a été proposé dans la diffusion d'autres sources canonique de l'Insee. 


```{python}
import duckdb 
from great_tables import *

tab = duckdb.sql("""
FROM read_csv_auto('/home/onyxia/work/jms-papier-parquet/consult_data_gouv_parquet.csv')
SELECT title, metrics_resources_downloads
WHERE organization_name='Institut national de la statistique et des études économiques (Insee)'
ORDER BY metrics_resources_downloads DESC
""").to_df()
GT(tab)
```

# Perspectives

Si ces premières expériences de diffusion de données aux formats `Parquet` ont permis d'illustrer l'apport de ce format pour les _data scientists_, statisticiens et géomaticiens utilisant des données détaillées, ce n'est pas la seule communauté pouvant bénéficier de ce format. Les spécialistes de _data visualisation_, une communauté plus proche du développement web, utilisant plutôt le langage `Javascript` peuvent également bénéficier de ce format. Grâce au portage de `DuckDB` en _Web Assembly_, il est possible de construire des _data visualisations_ réactives qui consomment des blocs de données, définis à partir d'une interface de sélection, d'un fichier `Parquet` plus large. Si ce dernier est bien organisé, par exemple avec des données ordonnées pour les sélecteurs mis dans l'interface, et des serveurs adéquats (qui autorisent les range requrst) il est possible d'avoir une consommation très efficace d'un fichier large. 

Si Parquet n'a pas vocation à être le seul format de diffusion du fait de la nature du public utilisateur du site insee.fr et de ses besoins majoritaires (des données plus ou moins agrégées prêtes à l'emploi, pas forcément lues depuis un logiicel statistique), on peut néanmoins considérer que dans de nombreux cas d'usage analytique de la donnée, celui-ci pourrait servir comme mode d'accès aux données alternatif aux API. 







